{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d7f55c",
   "metadata": {},
   "source": [
    "# **Natural Language Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16475303",
   "metadata": {},
   "source": [
    "## **NLP** application : \n",
    "- Text translation\n",
    "- Speech recognition\n",
    "- Natural language understanding\n",
    "- Sentiment analysis\n",
    "- Topic modelling\n",
    "- Summarization\n",
    "- Grammatical correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a3ce8",
   "metadata": {},
   "source": [
    "**NLTK** Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310e8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5cb0fa",
   "metadata": {},
   "source": [
    "## **Tokenization** :  is the process of splitting a text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "824273ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Ain't no sunshine when she's gone. And she's always gone too long. Anytime she goes away.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e381451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ai', \"n't\", 'no', 'sunshine', 'when', 'she', \"'s\", 'gone', '.', 'And', 'she', \"'s\", 'always', 'gone', 'too', 'long', '.', 'Anytime', 'she', 'goes', 'away', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(document)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6703deab",
   "metadata": {},
   "source": [
    "### remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e8e71ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ai', 'no', 'sunshine', 'when', 'she', 'gone', 'And', 'she', 'always', 'gone', 'too', 'long', 'Anytime', 'she', 'goes', 'away']\n"
     ]
    }
   ],
   "source": [
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b27c43",
   "metadata": {},
   "source": [
    "## remove stopwords\n",
    "\n",
    "Stop words are words that typically add no value to the text, but are only present for grammatical reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38de0fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22d69c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ai', 'sunshine', 'gone', 'And', 'always', 'gone', 'long', 'Anytime', 'goes', 'away']\n"
     ]
    }
   ],
   "source": [
    "tokens = [word for word in tokens if word not in eng_stopwords]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47546c92",
   "metadata": {},
   "source": [
    "## python is case sensitive\n",
    "'And' == 'and' # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "412db9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ai', 'sunshine', 'gone', 'always', 'gone', 'long', 'Anytime', 'goes', 'away']\n"
     ]
    }
   ],
   "source": [
    "tokens = [word for word in tokens if word.lower() not in eng_stopwords]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f115e57",
   "metadata": {},
   "source": [
    "## **Stemming** and **Lemmatization**\n",
    "\n",
    "Stemming and lemmatization are basically the action of keeping only the root of words.\n",
    "- Stemming : merely truncating the word\n",
    "- Lemmatization : use context to get the relevant root of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a5bb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connect connect connect\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(\"connection\"), stemmer.stem(\"connected\"), stemmer.stem(\"connective\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66477d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean mean is\n",
      "meaning meanness be\n"
     ]
    }
   ],
   "source": [
    "# but what if the words don't mean the same thing once truncated\n",
    "print(stemmer.stem(\"meaning\"), stemmer.stem(\"meanness\"), stemmer.stem(\"is\"))\n",
    "print(lemmatizer.lemmatize(\"meaning\", \"n\"), lemmatizer.lemmatize(\"meanness\", \"n\"), lemmatizer.lemmatize(\"is\", \"v\")) # better model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3f044",
   "metadata": {},
   "source": [
    "**POS** tagging : Part Of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1709e35",
   "metadata": {},
   "source": [
    "## **Ngrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ffcd9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4169bdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ai', \"n't\"),\n",
       " (\"n't\", 'no'),\n",
       " ('no', 'sunshine'),\n",
       " ('sunshine', 'when'),\n",
       " ('when', 'she'),\n",
       " ('she', \"'s\"),\n",
       " (\"'s\", 'gone'),\n",
       " ('gone', '.'),\n",
       " ('.', 'And'),\n",
       " ('And', 'she'),\n",
       " ('she', \"'s\"),\n",
       " (\"'s\", 'always'),\n",
       " ('always', 'gone'),\n",
       " ('gone', 'too'),\n",
       " ('too', 'long'),\n",
       " ('long', '.'),\n",
       " ('.', 'Anytime'),\n",
       " ('Anytime', 'she'),\n",
       " ('she', 'goes'),\n",
       " ('goes', 'away'),\n",
       " ('away', '.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 word per token\n",
    "tokens = word_tokenize(document)\n",
    "list(nltk.bigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91f8dedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ai', \"n't\", 'no'),\n",
       " (\"n't\", 'no', 'sunshine'),\n",
       " ('no', 'sunshine', 'when'),\n",
       " ('sunshine', 'when', 'she'),\n",
       " ('when', 'she', \"'s\"),\n",
       " ('she', \"'s\", 'gone'),\n",
       " (\"'s\", 'gone', '.'),\n",
       " ('gone', '.', 'And'),\n",
       " ('.', 'And', 'she'),\n",
       " ('And', 'she', \"'s\"),\n",
       " ('she', \"'s\", 'always'),\n",
       " (\"'s\", 'always', 'gone'),\n",
       " ('always', 'gone', 'too'),\n",
       " ('gone', 'too', 'long'),\n",
       " ('too', 'long', '.'),\n",
       " ('long', '.', 'Anytime'),\n",
       " ('.', 'Anytime', 'she'),\n",
       " ('Anytime', 'she', 'goes'),\n",
       " ('she', 'goes', 'away'),\n",
       " ('goes', 'away', '.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 word per token\n",
    "list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7241c2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ai', \"n't\", 'no', 'sunshine', 'when'),\n",
       " (\"n't\", 'no', 'sunshine', 'when', 'she'),\n",
       " ('no', 'sunshine', 'when', 'she', \"'s\"),\n",
       " ('sunshine', 'when', 'she', \"'s\", 'gone'),\n",
       " ('when', 'she', \"'s\", 'gone', '.'),\n",
       " ('she', \"'s\", 'gone', '.', 'And'),\n",
       " (\"'s\", 'gone', '.', 'And', 'she'),\n",
       " ('gone', '.', 'And', 'she', \"'s\"),\n",
       " ('.', 'And', 'she', \"'s\", 'always'),\n",
       " ('And', 'she', \"'s\", 'always', 'gone'),\n",
       " ('she', \"'s\", 'always', 'gone', 'too'),\n",
       " (\"'s\", 'always', 'gone', 'too', 'long'),\n",
       " ('always', 'gone', 'too', 'long', '.'),\n",
       " ('gone', 'too', 'long', '.', 'Anytime'),\n",
       " ('too', 'long', '.', 'Anytime', 'she'),\n",
       " ('long', '.', 'Anytime', 'she', 'goes'),\n",
       " ('.', 'Anytime', 'she', 'goes', 'away'),\n",
       " ('Anytime', 'she', 'goes', 'away', '.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n word per token\n",
    "list(nltk.ngrams(tokens, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14617284",
   "metadata": {},
   "source": [
    "## **BOW** : Bag of Words\n",
    "\n",
    "a bow is just a vector keeping track of how many times each word has been encountered in a text\n",
    "\n",
    "⚠️ It does not keep any information about the grammar or the order of the words in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f051987b",
   "metadata": {},
   "source": [
    "### the bow would be \n",
    "\"Nicolas loves to watch Disney movies but everybody loves Disney movies. Pierre loves football, unlike Nicolas.\"\n",
    "\n",
    "BOW = {Nicolas: 2, loves: 3, to: 1, watch: 1, Disney: 2, movies: 2,\n",
    "       but: 1, everybody: 1, Pierre: 1, football: 1, unlike: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66b17564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countVectorizer = CountVectorizer(max_features=1000, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "177e324e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 0, 2, 2, 1, 0, 1],\n",
       "       [0, 0, 1, 1, 1, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = [\"Nicolas loves to watch Disney movies but everybody loves Disney movies.\", \"Helene loves football, unlike Nicolas.\"]\n",
    "# We create the BOW, we also can directly remove the stopwords and the punctuation\n",
    "bow = countVectorizer.fit_transform(document).toarray()\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adc1c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c894e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['disney', 'everybody', 'football', 'helene', 'loves', 'movies',\n",
       "       'nicolas', 'unlike', 'watch'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the words associated to the vectors\n",
    "tokens = countVectorizer.get_feature_names_out()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659ade7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disney</th>\n",
       "      <th>everybody</th>\n",
       "      <th>football</th>\n",
       "      <th>helene</th>\n",
       "      <th>loves</th>\n",
       "      <th>movies</th>\n",
       "      <th>nicolas</th>\n",
       "      <th>unlike</th>\n",
       "      <th>watch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   disney  everybody  football  helene  loves  movies  nicolas  unlike  watch\n",
       "0       2          1         0       0      2       2        1       0      1\n",
       "1       0          0         1       1      1       0        1       1      0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=bow, columns=tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
